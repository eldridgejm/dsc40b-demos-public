{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$NN Classifiers\n",
    "\n",
    "In class, we saw that using $k$NN density estimation leads to a simple rule for classification: draw a ball of radius $r_k(x)$ around $x$, and return the label that occurs most often within the ball. This notebook will implement this simple idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moon data\n",
    "\n",
    "`sklearn` comes with some data out of the box. The following code generates two crescent moons, with some noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moons_features_array, moons_labels = sklearn.datasets.make_moons(200, noise=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`moons_features_array` is a 2-d NumPy array. You have experience working with 1-d NumPy arrays from DSC 10, but maybe not so much practice with 2-d arrays, so I'll go ahead and convert it to a list of 1-d arrays right off the bat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moons_features = list(moons_features_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`moons_labels` is a 1-d array of labels (either 0 or 1), telling us which crescent moon the point belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moons_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moons_x_1, moons_x_2 = moons_features_array.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(moons_x_1, moons_x_2, c=moons_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A yellow point has label 1, while a purple point has label 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classifier\n",
    "\n",
    "Now we write a function which will take in a point z, along with the data, and return a predicted label. The first thing the function must do is find all points which are within radius $r_k(z)$ of z; that is, it finds the $k$ closest points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_closest_points(z, features, k=3):\n",
    "    \"\"\"Find the k closest points to z in the features.\n",
    "    \n",
    "    Returns a list of pairs. Each pair contains:\n",
    "    \n",
    "        (distance to z, index of point)\n",
    "    \n",
    "    \"\"\"\n",
    "    # find the distance from z to every point\n",
    "    distances = []\n",
    "    for ix, x in enumerate(features):\n",
    "        distance = np.sum((x - z)**2)**(1/2)\n",
    "        distances.append((distance, ix))\n",
    "    \n",
    "    return sorted(distances)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check and make sure this function is doing what we expect. The below will plot the point $z$ in red, a circle of radius $r_k(z)$ around $z$, and all points within the circle in orange. Try changing $z$ and $k$ and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [0, 0]\n",
    "k = 6\n",
    "\n",
    "plt.scatter(moons_x_1, moons_x_2)\n",
    "plt.scatter(*z, color='red')\n",
    "\n",
    "closest = k_closest_points(z, moons_features, k=k)\n",
    "\n",
    "for _, ix in closest:\n",
    "    x, y = moons_features[ix]\n",
    "    plt.scatter(x, y, color='orange')\n",
    "    \n",
    "r = closest[-1][0]\n",
    "circle = plt.Circle(z, r, fill=False)\n",
    "plt.gca().add_artist(circle)\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our classifier function. It should return the label that is found most frequently within the circle. Because the labels are either 0 or 1 here, this amounts to summing up the labels and returning 1 if the sum  is greater than k/2. Try changing $z$ and $k$ and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classify(z, features, labels, k=3):\n",
    "    closest = k_closest_points(z, features, k)\n",
    "    votes = [labels[ix] for _,ix in closest]\n",
    "    return int(sum(votes) > k/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [0, 1]\n",
    "k = 6\n",
    "\n",
    "plt.scatter(moons_x_1, moons_x_2, c=moons_labels)\n",
    "plt.scatter(*z, color='red')\n",
    "\n",
    "closest = k_closest_points(z, moons_features, k=k)\n",
    "    \n",
    "r = closest[-1][0]\n",
    "circle = plt.Circle(z, r, fill=False)\n",
    "plt.gca().add_artist(circle)\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "prediction = knn_classify(z, moons_features, moons_labels, k=k)\n",
    "print('Prediction:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data\n",
    "\n",
    "Now let's look at a slightly larger and more interesting dataset: the MNIST handwritten image dataset. We'll use this as an opportunity to re-write the kNN classifier \"correctly\", using fast NumPy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = np.load('mnist.npz')\n",
    "mnist_train_features = mnist_data['train'].T.astype(float)\n",
    "mnist_train_labels = mnist_data['train_labels'].flatten()\n",
    "mnist_test_features = mnist_data['test'].T.astype(float)\n",
    "mnist_test_labels = mnist_data['test_labels'].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now in a $60,000 \\times 784$ array. There are 60,000 examples, each being a 784-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these vectors is actually a 28x28 image, \"flattened\" into a vector. We can reshape and visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mnist_train_features[33_000].reshape(28, -1), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The (fast) classifier\n",
    "\n",
    "Now we will re-write `k_closest_points` and `knn_classify` from last lecture, but faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance\n",
    "\n",
    "def k_closest_points(z, features, k=3):\n",
    "    distances = scipy.spatial.distance_matrix([z], features).flatten()\n",
    "    return np.argpartition(distances, k)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out. We saw that vector #33,000 is a five. What are its closest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "closest = k_closest_points(mnist_train_features[33_000], mnist_train_features, k=7)\n",
    "for ix in closest:\n",
    "    plt.figure()\n",
    "    plt.imshow(mnist_train_features[ix].reshape(28, -1), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we re-write the classification function to use fast NumPy functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(z, features, labels, k=5):\n",
    "    closest_ix = k_closest_points(z, features, k)\n",
    "    closest_labels = labels[closest_ix]\n",
    "    values, counts = np.unique(closest_labels, return_counts=True)\n",
    "    return values[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out on unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 6500\n",
    "\n",
    "plt.imshow(mnist_test_features[ix].reshape(-1, 28), cmap='gray')\n",
    "\n",
    "prediction = knn_classifier(\n",
    "    mnist_test_features[ix], \n",
    "    mnist_train_features, \n",
    "    mnist_train_labels\n",
    ")\n",
    "\n",
    "print('Prediction:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the classifier on 100 random unseen examples. How many does it get right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_knn_classification_error(train_features, train_labels, test_features, test_labels, k=3, trials=50):\n",
    "    correct = 0\n",
    "    for i in range(trials):\n",
    "        ix = np.random.randint(len(test_features))\n",
    "        prediction = knn_classifier(\n",
    "            test_features[ix], \n",
    "            train_features, \n",
    "            train_labels\n",
    "        )\n",
    "        if prediction == test_labels[ix]:\n",
    "            correct += 1\n",
    "    return correct / trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_knn_classification_error(\n",
    "    mnist_train_features, \n",
    "    mnist_train_labels, \n",
    "    mnist_test_features, \n",
    "    mnist_test_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding noisy dimensions\n",
    "\n",
    "The MNIST data is nice in two ways: it has very little noise, and there aren't spurious dimensions to \"confuse\" our classifier. Let's add a bunch of noisy dimensions and see how our $k$NN classifier performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_NEW_ROWS = 28*3\n",
    "NOISE_MU = 200\n",
    "NOISE_SIGMA = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noisy_dimensions(data):\n",
    "    noisy_data = np.pad(data, [[0, 0], [0, NUMBER_OF_NEW_ROWS * 28]], 'constant')\n",
    "    appended_shape = (noisy_data.shape[0], NUMBER_OF_NEW_ROWS*28)\n",
    "    noisy_data += np.random.normal(NOISE_MU, NOISE_SIGMA, noisy_data.shape)\n",
    "    return np.clip(noisy_data, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_train_features = add_noisy_dimensions(mnist_train_features)\n",
    "noisy_test_features = add_noisy_dimensions(mnist_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(noisy_train_features[33_000].reshape(-1, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does adding noise affect the nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ix in k_closest_points(noisy_train_features[33_000], noisy_train_features, k=7):\n",
    "    plt.figure()\n",
    "    plt.imshow(noisy_train_features[ix].reshape(-1, 28))\n",
    "    plt.title(f'Label: {mnist_train_labels[ix]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional noisy dimensions must adversely affect the accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_knn_classification_error(\n",
    "    noisy_train_features, \n",
    "    mnist_train_labels, \n",
    "    noisy_test_features, \n",
    "    mnist_test_labels,\n",
    "    trials=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How can we remove extra noisy dimensions from the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0: Elevation / quantitative /meters / Elevation in meters \n",
    "- 1: Aspect / quantitative / azimuth / Aspect in degrees azimuth \n",
    "- 2: Slope / quantitative / degrees / Slope in degrees \n",
    "- 3: Horizontal_Distance_To_Hydrology / quantitative / meters / Horz Dist to nearest surface water features \n",
    "- 4: Vertical_Distance_To_Hydrology / quantitative / meters / Vert Dist to nearest surface water features \n",
    "- 5: Horizontal_Distance_To_Roadways / quantitative / meters / Horz Dist to nearest roadway \n",
    "- 6: Hillshade_9am / quantitative / 0 to 255 index / Hillshade index at 9am, summer solstice \n",
    "- 7: Hillshade_Noon / quantitative / 0 to 255 index / Hillshade index at noon, summer soltice \n",
    "- 8: Hillshade_3pm / quantitative / 0 to 255 index / Hillshade index at 3pm, summer solstice \n",
    "- 9: Horizontal_Distance_To_Fire_Points / quantitative / meters / Horz Dist to nearest wildfire ignition points \n",
    "- 10-13: Wilderness_Area (4 binary columns) / qualitative / 0 (absence) or 1 (presence) / Wilderness area designation\n",
    "- 14-53: Soil_Type (40 binary columns) / qualitative / 0 (absence) or 1 (presence) / Soil Type designation \n",
    "- 54: Cover_Type (7 types) / integer / 1 to 7 / Forest Cover Type designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('covtype.npz')['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized = (data - data.mean(axis=0)) / data.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(standardized.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov[np.diag_indices_from(cov)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "plt.matshow(np.abs(cov[:K,:K]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(np.abs(cov))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
